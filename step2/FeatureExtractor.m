function [x80, state] = FeatureExtractor(x0, params, varargin)
%FEATUREEXTRACTOR Function implementing an imported ONNX network.
%
% THIS FILE WAS AUTO-GENERATED BY importONNXFunction.
% ONNX Operator Set Version: 9
%
% Variable names in this function are taken from the original ONNX file.
%
% [X80] = FeatureExtractor(X0, PARAMS)
%			- Evaluates the imported ONNX network FEATUREEXTRACTOR with input(s)
%			X0 and the imported network parameters in PARAMS. Returns
%			network output(s) in X80.
%
% [X80, STATE] = FeatureExtractor(X0, PARAMS)
%			- Additionally returns state variables in STATE. When training,
%			use this form and set TRAINING to true.
%
% [__] = FeatureExtractor(X0, PARAMS, 'NAME1', VAL1, 'NAME2', VAL2, ...)
%			- Specifies additional name-value pairs described below:
%
% 'Training'
% 			Boolean indicating whether the network is being evaluated for
%			prediction or training. If TRAINING is true, state variables
%			will be updated.
%
% 'InputDataPermutation'
%			'auto' - Automatically attempt to determine the permutation
%			 between the dimensions of the input data and the dimensions of
%			the ONNX model input. For example, the permutation from HWCN
%			(MATLAB standard) to NCHW (ONNX standard) uses the vector
%			[4 3 1 2]. See the documentation for IMPORTONNXFUNCTION for
%			more information about automatic permutation.
%
%			'none' - Input(s) are passed in the ONNX model format. See 'Inputs'.
%
%			numeric vector - The permutation vector describing the
%			transformation between input data dimensions and the expected
%			ONNX input dimensions.%
%			cell array - If the network has multiple inputs, each cell
%			contains 'auto', 'none', or a numeric vector.
%
% 'OutputDataPermutation'
%			'auto' - Automatically attempt to determine the permutation
%			between the dimensions of the output and a conventional MATLAB
%			dimension ordering. For example, the permutation from NC (ONNX
%			standard) to CN (MATLAB standard) uses the vector [2 1]. See
%			the documentation for IMPORTONNXFUNCTION for more information
%			about automatic permutation.
%
%			'none' - Return output(s) as given by the ONNX model. See 'Outputs'.
%
%			numeric vector - The permutation vector describing the
%			transformation between the ONNX output dimensions and the
%			desired output dimensions.%
%			cell array - If the network has multiple outputs, each cell
%			contains 'auto', 'none' or a numeric vector.
%
% Inputs:
% -------
% X0
%			- Input(s) to the ONNX network.
%			  The input size(s) expected by the ONNX file are:
%				  X0:		[1, 1, 150, 220]				Type: FLOAT
%			  By default, the function will try to permute the input(s)
%			  into this dimension ordering. If the default is incorrect,
%			  use the 'InputDataPermutation' argument to control the
%			  permutation.
%
%
% PARAMS	- Network parameters returned by 'importONNXFunction'.
%
%
% Outputs:
% --------
% X80
%			- Output(s) of the ONNX network.
%			  Without permutation, the size(s) of the outputs are:
%				  X80:		[1, 2048]				Type: FLOAT
%			  By default, the function will try to permute the output(s)
%			  from this dimension ordering into a conventional MATLAB
%			  ordering. If the default is incorrect, use the
%			  'OutputDataPermutation' argument to control the permutation.
%
% STATE		- (Optional) State variables. When TRAINING is true, these will
% 			  have been updated from the original values in PARAMS.State.
%
%
%  See also importONNXFunction

% Preprocess the input data and arguments:
[x0, Training, outputDataPerms, anyDlarrayInputs] = preprocessInput(x0, params, varargin{:});
% Put all variables into a single struct to implement dynamic scoping:
[Vars, NumDims] = packageVariables(params, {'x0'}, {x0}, [4]);
% Call the top-level graph function:
[x80, NumDims.x80, state] = torch_jit_exportGraph1000(x0, NumDims.x0, Vars, NumDims, Training, params.State);
% Postprocess the output data
[x80] = postprocessOutput(x80, outputDataPerms, anyDlarrayInputs, Training, varargin{:});
end

function [x80, x80NumDims1034, state] = torch_jit_exportGraph1000(x0, x0NumDims1033, Vars, NumDims, Training, state)
% Function implementing the graph 'torch_jit_exportGraph1000'
% Update Vars and NumDims from the graph's formal input parameters. Note that state variables are already in Vars.
Vars.x0 = x0;
NumDims.x0 = x0NumDims1033;

% Execute the operators:
% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x43] = prepareConvArgs(Vars.x1, '', Vars.ConvStride1001, Vars.ConvDilationFactor1002, Vars.ConvPadding1003, 1, NumDims.x0, NumDims.x1);
Vars.x43 = dlconv(Vars.x0, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x44, NumDims.x4, NumDims.x5] = prepareBatchNormalizationArgs(Vars.x3, Vars.x2, Vars.x4, Vars.x5, NumDims.x43, NumDims.x4, NumDims.x5);
if Training
    [Vars.x44, dsmean, dsvar] = batchnorm(Vars.x43, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.x4 = dlarray(dsmean);
    Vars.x5 = dlarray(dsvar);
else
    Vars.x44 = batchnorm(Vars.x43, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.x4 = Vars.x4;
state.x5 = Vars.x5;

% Relu:
Vars.x45 = relu(Vars.x44);
NumDims.x45 = NumDims.x44;

% MaxPool:
[poolsize, stride, padding, dataFormat, NumDims.x46] = prepareMaxPool8Args(Vars.MaxPoolPoolSize1004, Vars.MaxPoolStride1005, Vars.MaxPoolPadding1006, NumDims.x45);
Vars.x46 = maxpool(Vars.x45, poolsize, 'Stride', stride, 'Padding', padding, 'DataFormat', dataFormat);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x47] = prepareConvArgs(Vars.x7, '', Vars.ConvStride1007, Vars.ConvDilationFactor1008, Vars.ConvPadding1009, 1, NumDims.x46, NumDims.x7);
Vars.x47 = dlconv(Vars.x46, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x48, NumDims.x10, NumDims.x11] = prepareBatchNormalizationArgs(Vars.x9, Vars.x8, Vars.x10, Vars.x11, NumDims.x47, NumDims.x10, NumDims.x11);
if Training
    [Vars.x48, dsmean, dsvar] = batchnorm(Vars.x47, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.x10 = dlarray(dsmean);
    Vars.x11 = dlarray(dsvar);
else
    Vars.x48 = batchnorm(Vars.x47, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.x10 = Vars.x10;
state.x11 = Vars.x11;

% Relu:
Vars.x49 = relu(Vars.x48);
NumDims.x49 = NumDims.x48;

% MaxPool:
[poolsize, stride, padding, dataFormat, NumDims.x50] = prepareMaxPool8Args(Vars.MaxPoolPoolSize1010, Vars.MaxPoolStride1011, Vars.MaxPoolPadding1012, NumDims.x49);
Vars.x50 = maxpool(Vars.x49, poolsize, 'Stride', stride, 'Padding', padding, 'DataFormat', dataFormat);

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x51] = prepareConvArgs(Vars.x13, '', Vars.ConvStride1013, Vars.ConvDilationFactor1014, Vars.ConvPadding1015, 1, NumDims.x50, NumDims.x13);
Vars.x51 = dlconv(Vars.x50, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x52, NumDims.x16, NumDims.x17] = prepareBatchNormalizationArgs(Vars.x15, Vars.x14, Vars.x16, Vars.x17, NumDims.x51, NumDims.x16, NumDims.x17);
if Training
    [Vars.x52, dsmean, dsvar] = batchnorm(Vars.x51, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.x16 = dlarray(dsmean);
    Vars.x17 = dlarray(dsvar);
else
    Vars.x52 = batchnorm(Vars.x51, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.x16 = Vars.x16;
state.x17 = Vars.x17;

% Relu:
Vars.x53 = relu(Vars.x52);
NumDims.x53 = NumDims.x52;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x54] = prepareConvArgs(Vars.x19, '', Vars.ConvStride1016, Vars.ConvDilationFactor1017, Vars.ConvPadding1018, 1, NumDims.x53, NumDims.x19);
Vars.x54 = dlconv(Vars.x53, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x55, NumDims.x22, NumDims.x23] = prepareBatchNormalizationArgs(Vars.x21, Vars.x20, Vars.x22, Vars.x23, NumDims.x54, NumDims.x22, NumDims.x23);
if Training
    [Vars.x55, dsmean, dsvar] = batchnorm(Vars.x54, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.x22 = dlarray(dsmean);
    Vars.x23 = dlarray(dsvar);
else
    Vars.x55 = batchnorm(Vars.x54, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.x22 = Vars.x22;
state.x23 = Vars.x23;

% Relu:
Vars.x56 = relu(Vars.x55);
NumDims.x56 = NumDims.x55;

% Conv:
[weights, bias, stride, dilationFactor, padding, dataFormat, NumDims.x57] = prepareConvArgs(Vars.x25, '', Vars.ConvStride1019, Vars.ConvDilationFactor1020, Vars.ConvPadding1021, 1, NumDims.x56, NumDims.x25);
Vars.x57 = dlconv(Vars.x56, weights, bias, 'Stride', stride, 'DilationFactor', dilationFactor, 'Padding', padding, 'DataFormat', dataFormat);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x58, NumDims.x28, NumDims.x29] = prepareBatchNormalizationArgs(Vars.x27, Vars.x26, Vars.x28, Vars.x29, NumDims.x57, NumDims.x28, NumDims.x29);
if Training
    [Vars.x58, dsmean, dsvar] = batchnorm(Vars.x57, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.x28 = dlarray(dsmean);
    Vars.x29 = dlarray(dsvar);
else
    Vars.x58 = batchnorm(Vars.x57, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.x28 = Vars.x28;
state.x29 = Vars.x29;

% Relu:
Vars.x59 = relu(Vars.x58);
NumDims.x59 = NumDims.x58;

% MaxPool:
[poolsize, stride, padding, dataFormat, NumDims.x60] = prepareMaxPool8Args(Vars.MaxPoolPoolSize1022, Vars.MaxPoolStride1023, Vars.MaxPoolPadding1024, NumDims.x59);
Vars.x60 = maxpool(Vars.x59, poolsize, 'Stride', stride, 'Padding', padding, 'DataFormat', dataFormat);

% Shape:
[Vars.x62, NumDims.x62] = onnxShape(Vars.x60, NumDims.x60);

% Gather:
[Vars.x63, NumDims.x63] = onnxGather(Vars.x62, Vars.x61, 0, NumDims.x62, NumDims.x61);

% Unsqueeze:
[shape, NumDims.x65] = prepareUnsqueezeArgs(Vars.x63, Vars.UnsqueezeAxes1025, NumDims.x63);
Vars.x65 = reshape(Vars.x63, shape);

% Unsqueeze:
[shape, NumDims.x66] = prepareUnsqueezeArgs(Vars.x64, Vars.UnsqueezeAxes1026, NumDims.x64);
Vars.x66 = reshape(Vars.x64, shape);

% Concat:
[dim, NumDims.x67] = prepareConcatArgs(0, [NumDims.x65, NumDims.x66]);
Vars.x67 = cat(dim, Vars.x65, Vars.x66);

% Reshape:
[shape, NumDims.x68] = prepareReshapeArgs(Vars.x60, Vars.x67, NumDims.x60);
Vars.x68 = reshape(Vars.x60, shape{:});

% Transpose:
[perm, NumDims.x69] = prepareTransposeArgs(Vars.TransposePerm1027, NumDims.x31);
if ~isempty(perm)
    Vars.x69 = permute(Vars.x31, perm);
end

% MatMul:
[Vars.x70, NumDims.x70] = onnxMatMul(Vars.x68, Vars.x69, NumDims.x68, NumDims.x69);

% Unsqueeze:
[shape, NumDims.x71] = prepareUnsqueezeArgs(Vars.x70, Vars.UnsqueezeAxes1028, NumDims.x70);
Vars.x71 = reshape(Vars.x70, shape);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x72, NumDims.x34, NumDims.x35] = prepareBatchNormalizationArgs(Vars.x33, Vars.x32, Vars.x34, Vars.x35, NumDims.x71, NumDims.x34, NumDims.x35);
if Training
    [Vars.x72, dsmean, dsvar] = batchnorm(Vars.x71, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.x34 = dlarray(dsmean);
    Vars.x35 = dlarray(dsvar);
else
    Vars.x72 = batchnorm(Vars.x71, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.x34 = Vars.x34;
state.x35 = Vars.x35;

% Squeeze:
[Vars.x73, NumDims.x73] = onnxSqueeze(Vars.x72, Vars.SqueezeAxes1029, NumDims.x72);

% Relu:
Vars.x74 = relu(Vars.x73);
NumDims.x74 = NumDims.x73;

% Transpose:
[perm, NumDims.x75] = prepareTransposeArgs(Vars.TransposePerm1030, NumDims.x37);
if ~isempty(perm)
    Vars.x75 = permute(Vars.x37, perm);
end

% MatMul:
[Vars.x76, NumDims.x76] = onnxMatMul(Vars.x74, Vars.x75, NumDims.x74, NumDims.x75);

% Unsqueeze:
[shape, NumDims.x77] = prepareUnsqueezeArgs(Vars.x76, Vars.UnsqueezeAxes1031, NumDims.x76);
Vars.x77 = reshape(Vars.x76, shape);

% BatchNormalization:
[offset, scale, datasetMean, datasetVariance, dataFormat, NumDims.x78, NumDims.x40, NumDims.x41] = prepareBatchNormalizationArgs(Vars.x39, Vars.x38, Vars.x40, Vars.x41, NumDims.x77, NumDims.x40, NumDims.x41);
if Training
    [Vars.x78, dsmean, dsvar] = batchnorm(Vars.x77, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
    Vars.x40 = dlarray(dsmean);
    Vars.x41 = dlarray(dsvar);
else
    Vars.x78 = batchnorm(Vars.x77, offset, scale, datasetMean, datasetVariance, 'Epsilon', 0.000010, 'DataFormat', dataFormat);
end
state.x40 = Vars.x40;
state.x41 = Vars.x41;

% Squeeze:
[Vars.x79, NumDims.x79] = onnxSqueeze(Vars.x78, Vars.SqueezeAxes1032, NumDims.x78);

% Relu:
Vars.x80 = relu(Vars.x79);
NumDims.x80 = NumDims.x79;

% Set graph output arguments from Vars and NumDims:
x80 = Vars.x80;
x80NumDims1034 = NumDims.x80;
% Set output state from Vars:
state = updateStruct(state, Vars);
end

function [inputDataPerms, outputDataPerms, Training] = parseInputs(x0, numDataOutputs, params, varargin)
% Function to validate inputs to FeatureExtractor:
p = inputParser;
isValidArrayInput = @(x)isnumeric(x) || isstring(x);
isValidONNXParameters = @(x)isa(x, 'ONNXParameters');
addRequired(p, 'x0', isValidArrayInput);
addRequired(p, 'params', isValidONNXParameters);
addParameter(p, 'InputDataPermutation', 'auto');
addParameter(p, 'OutputDataPermutation', 'auto');
addParameter(p, 'Training', false);
parse(p, x0, params, varargin{:});
inputDataPerms = p.Results.InputDataPermutation;
outputDataPerms = p.Results.OutputDataPermutation;
Training = p.Results.Training;
if isnumeric(inputDataPerms)
    inputDataPerms = {inputDataPerms};
end
if isstring(inputDataPerms) && isscalar(inputDataPerms) || ischar(inputDataPerms)
    inputDataPerms = repmat({inputDataPerms},1,1);
end
if isnumeric(outputDataPerms)
    outputDataPerms = {outputDataPerms};
end
if isstring(outputDataPerms) && isscalar(outputDataPerms) || ischar(outputDataPerms)
    outputDataPerms = repmat({outputDataPerms},1,numDataOutputs);
end
end

function [x0, Training, outputDataPerms, anyDlarrayInputs] = preprocessInput(x0, params, varargin)
% Parse input arguments
[inputDataPerms, outputDataPerms, Training] = parseInputs(x0, 1, params, varargin{:});
anyDlarrayInputs = any(cellfun(@(x)isa(x, 'dlarray'), {x0}));
% Make the input variables into unlabelled dlarrays:
x0 = makeUnlabeledDlarray(x0);
% Permute inputs if requested:
x0 = permuteInputVar(x0, inputDataPerms{1}, 4);
% Check input size(s):
checkInputSize(size(x0), {1 1 150 220}, "x0");
end

function [x80] = postprocessOutput(x80, outputDataPerms, anyDlarrayInputs, Training, varargin)
% Set output type:
if ~anyDlarrayInputs && ~Training
    x80 = extractdata(x80);
end
% Permute outputs if requested:
x80 = permuteOutputVar(x80, outputDataPerms{1}, 2);
end


%% dlarray functions implementing ONNX operators:

function [Y, numDimsY] = onnxGather(X, ONNXIdx, ONNXAxis, numDimsX, numDimsIdx)
% Function implementing the ONNX Gather operator

% In ONNX, 'Gather' first indexes into dimension ONNXAxis of data, using
% the contents of ONNXIdx as the indices. Then, it reshapes the ONNXAxis
% into the shape of ONNXIdx.
%   Example 1:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [6 7], and axis=1.
% The result has shape [2 6 7 4 5].
%   Example 2:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [6], and axis=1.
% The result has shape [2 6 4 5].
%   Example 3:
% Suppose data has shape [2 3 4 5], ONNXIdx has shape [] (a scalar), and axis=1.
% The result has shape [2 4 5].
%
% Since we're using reverse indexing relative to ONNX, in this function
% data and ONNXIdx both have reversed dimension ordering.
numDimsY = numDimsIdx + (numDimsX - 1);
if isempty(X)
    Y = X;
    return;
end
% (1) First, do the subsref part of Gather
if ONNXAxis<0
    ONNXAxis = ONNXAxis + numDimsX;                                 % Axis can be negative. Convert it to its positive equivalent.
end
% ONNXIdx is origin-0 in ONNX, so add 1
mlIdx  = extractdata(ONNXIdx) + 1;
% Convert axis to DLT. ONNXAxis is origin 0 and we index from the end
mlAxis = numDimsX - ONNXAxis;
% Use subsref to index into data
Indices.subs = repmat({':'}, 1, numDimsX);
Indices.subs{mlAxis} = mlIdx(:);                                          % Index as a column to ensure the output is 1-D in the indexed dimension (for now).
Indices.type = '()';
Y = subsref(X, Indices);
% (2) Now do the reshaping part of Gather
shape = size(Y, 1:numDimsX);
if numDimsIdx == 0
    % Delete the indexed dimension
    shape(mlAxis) = [];
elseif numDimsIdx > 1
    % Reshape the indexed dimension into the shape of ONNXIdx
    shape = [shape(1:mlAxis-1) size(ONNXIdx, 1:numDimsIdx) shape(mlAxis+1:end)];
end
% Extend the shape to 2D so it's valid MATLAB
if numel(shape) < 2
    shape = [shape ones(1,2-numel(shape))];
end
Y = reshape(Y, shape);
end

function [D, numDimsD] = onnxMatMul(A, B, numDimsA, numDimsB)
% Implements the ONNX MatMul operator.

% If either arg is more than 2D, loop over all dimensions before the final
% 2. Inside the loop, perform matrix multiplication.

% If B is 1-D, temporarily extend it to a row vector
if numDimsB==1
    B = B(:)';
end
maxNumDims = max(numDimsA, numDimsB);
numDimsD = maxNumDims;
if maxNumDims > 2
    % sizes of matrices to be multiplied
    matSizeA        = size(A, 1:2);
    matSizeB        = size(B, 1:2);
    % size of the stack of matrices
    stackSizeA      = size(A, 3:maxNumDims);
    stackSizeB      = size(B, 3:maxNumDims);
    % final stack size
    resultStackSize = max(stackSizeA, stackSizeB);
    % full implicitly-expanded sizes
    fullSizeA       = [matSizeA resultStackSize];
    fullSizeB       = [matSizeB resultStackSize];
    resultSize      = [matSizeB(1) matSizeA(2) resultStackSize];
    % Repmat A and B up to the full stack size using implicit expansion
    A = A + zeros(fullSizeA);
    B = B + zeros(fullSizeB);
    % Reshape A and B to flatten the stack dims (all dims after the first 2)
    A2 = reshape(A, size(A,1), size(A,2), []);
    B2 = reshape(B, size(B,1), size(B,2), []);
    % Iterate down the stack dim, doing the 2d matrix multiplications
    D2 = zeros([matSizeB(1), matSizeA(2), size(A2,3)], 'like', A);
    for i = size(A2,3):-1:1
        D2(:,:,i) = B2(:,:,i) * A2(:,:,i);
    end
    % Reshape D2 to the result size (unflatten the stack dims)
    D = reshape(D2, resultSize);
else
    D = B * A;
    if numDimsA==1 || numDimsB==1
        D = D(:);
        numDimsD = 1;
    end
end
end

function [Y, numDimsY] = onnxShape(X, numDimsX)
% Implements the ONNX Shape operator
% Return the reverse ONNX shape as a 1D column vector
switch numDimsX
    case 0
        if isempty(X)
            Y = dlarray(0);
        else
            Y = dlarray(1);
        end
    case 1
        if isempty(X)
            Y = dlarray(0);
        else
            Y = dlarray(size(X,1));
        end
    otherwise
        Y = dlarray(fliplr(size(X, 1:numDimsX))');
end
numDimsY = 1;
end

function [Y, numDimsY] = onnxSqueeze(X, ONNXAxes, numDimsX)
% Implements the ONNX Squeeze operator
if numDimsX == 0
    Y = X;
    numDimsY = numDimsX;
else
    % Find the new ONNX shape
    curOShape = size(X, numDimsX:-1:1);
    if isempty(ONNXAxes)
        newOShape = curOShape(curOShape ~= 1);
    else
        ONNXAxes(ONNXAxes<0) = ONNXAxes(ONNXAxes<0) + numDimsX;
        newOShape = curOShape;
        newOShape(ONNXAxes+1) = [];
    end
    % Get numDimsY from ONNX shape
    numDimsY  = numel(newOShape);
    newMShape = [fliplr(newOShape) ones(1, 2-numDimsY)];    % Append 1's to shape if numDims<2
    Y         = reshape(X, newMShape);
end
end

function [offset, scale, datasetMean, datasetVariance, dataFormat, numDimsY, numDimsDatasetMean, numDimsDatasetVariance] = prepareBatchNormalizationArgs(...
    offset, scale, datasetMean, datasetVariance, numDimsX, numDimsDatasetMean, numDimsDatasetVariance)
% Prepares arguments for implementing the ONNX BatchNormalization operator
offset = dlarray(offset,'C');
scale = dlarray(scale,'C');
datasetMean = extractdata(datasetMean);
datasetVariance = extractdata(datasetVariance);
datasetVariance(datasetVariance <= 0) = realmin('single');  % Set nonpositive variance components to a value below eps('single')
dataFormat = [repmat('U', 1, numDimsX-2), 'CB'];
numDimsY = numDimsX;
end

function [DLTAxis, numDimsY] = prepareConcatArgs(ONNXAxis, numDimsXs)
% Prepares arguments for implementing the ONNX Concat operator
numDimsY = numDimsXs(1);
if ONNXAxis<0
    ONNXAxis = ONNXAxis + numDimsY;
end
DLTAxis = numDimsY - ONNXAxis;
end

function [weights, bias, stride, dilationFactor, padding, dataFormat, numDimsY] = prepareConvArgs(...
    weights, bias, stride, dilationFactor, padding, numWtGroups, numDimsX, numDimsW)
% Prepares arguments for implementing the ONNX Conv operator

% Weights: The ONNX weight dim is Fcxyz..., where c=C/G, G is numGroups,
% and xyz... are spatial dimensions. DLT "weights" here is the flip of
% that, or ...zyxcF. dlconv requires ...zyxcfG, where f=F/G. So reshape to
% split the last dimension.
sizeW    = size(weights, 1:numDimsW);
F        = sizeW(end);
newWSize = [sizeW(1:numDimsW-1), F/numWtGroups, numWtGroups];
weights  = reshape(weights, newWSize);
% bias
if isempty(bias)
    bias = 0;
end
bias = dlarray(bias(:),'CU');
% Derive missing default attributes from weight tensor
numSpatialDims = numDimsW-2;
if isempty(padding)
    padding = zeros(1, 2*numSpatialDims);
end
if isempty(stride)
    stride = ones(1,numSpatialDims);
end
if isempty(dilationFactor)
    dilationFactor = ones(1,numSpatialDims);
end
% Make the attributes non-dlarrays:
if isa(stride, 'dlarray')
    stride = extractdata(stride);
end
if isa(dilationFactor, 'dlarray')
    dilationFactor = extractdata(dilationFactor);
end
if isa(padding, 'dlarray')
    padding = extractdata(padding);
end
% Make the attributes double row vectors, and flip their dimension ordering
% to reverse-onnx:
stride = fliplr(double(stride(:)'));
dilationFactor = fliplr(double(dilationFactor(:)'));
if isnumeric(padding)       % padding can be "same"
    % ONNX: [x1_begin, ..., xn_begin, x1_end, ...,xn_end]
    % DLT:  [xn_begin, ..., x1_begin;
    %        xn_end, ..., x1_end]       (Note the lrflip and semicolon)
    padding = fliplr(transpose(reshape(padding, [], 2)));
end
% Set dataformat and numdims
dataFormat = [repmat('S', 1, numDimsX-2) 'CB'];
numDimsY = numDimsX;
end

function [poolsize, stride, padding, dataFormat, numDimsY, numDimsIndices] = prepareMaxPool8Args(poolsize, stride, padding, numDimsX)
% Prepares arguments for implementing the ONNX MaxPool-8 operator
poolsize    = fliplr(extractdata(poolsize(:)'));
stride      = fliplr(extractdata(stride(:)'));
% padding
if isa(padding, 'dlarray')
    padding = extractdata(padding);
end
if isnumeric(padding)
    % ONNX: [x1_begin, ..., xn_begin, x1_end, ...,xn_end]
    % DLT:  [xn_begin, ..., x1_begin;
    %        xn_end, ..., x1_end]       (Note the lrflip and semicolon)
    padding = fliplr(transpose(reshape(padding, [], 2)));
end
dataFormat  = [repmat('S', 1, numDimsX-2) 'CB'];
numDimsY    = numDimsX;
numDimsIndices = numDimsX;                      % New in opset 8
end

function [DLTShape, numDimsY] = prepareReshapeArgs(X, ONNXShape, numDimsX)
% Prepares arguments for implementing the ONNX Reshape operator
ONNXShape = flip(extractdata(ONNXShape));            % First flip the shape to make it correspond to the dimensions of X.
% In ONNX, 0 means "unchanged", and -1 means "infer". In DLT, there is no
% "unchanged", and [] means "infer".
DLTShape = num2cell(ONNXShape);                      % Make a cell array so we can include [].
% Replace zeros with the actual size
if any(ONNXShape==0)
    i0 = find(ONNXShape==0);
    DLTShape(i0) = num2cell(size(X, numDimsX - numel(ONNXShape) + i0));  % right-align the shape vector and dims
end
if any(ONNXShape == -1)
    % Replace -1 with []
    i = ONNXShape == -1;
    DLTShape{i} = [];
end
if numel(DLTShape)==1
    DLTShape = [DLTShape 1];
end
numDimsY = numel(ONNXShape);
end

function [perm, numDimsA] = prepareTransposeArgs(ONNXPerm, numDimsA)
% Prepares arguments for implementing the ONNX Transpose operator
if numDimsA <= 1        % Tensors of numDims 0 or 1 are unchanged by ONNX Transpose.
    perm = [];
else
    if isempty(ONNXPerm)        % Empty ONNXPerm means reverse the dimensions.
        perm = numDimsA:-1:1;
    else
        perm = numDimsA-flip(ONNXPerm);
    end
end
end

function [newShape, numDimsY] = prepareUnsqueezeArgs(X, ONNXAxes, numDimsX)
% Prepares arguments for implementing the ONNX Unsqueeze operator

% ONNX axes are origin 0
ONNXAxes = extractdata(ONNXAxes);
ONNXAxes(ONNXAxes<0) = ONNXAxes(ONNXAxes<0) + numDimsX;
ONNXAxes = sort(ONNXAxes);                                              % increasing order
numDimsY = numDimsX + numel(ONNXAxes);
if numDimsY == 1
    newShape = size(X);
else
    DLTAxes  = flip(numDimsY - ONNXAxes);                                  % increasing order
    newShape = ones(1, numDimsY);
    posToSet = setdiff(1:numDimsY, DLTAxes, 'stable');
    newShape(posToSet) = size(X, 1:numel(posToSet));
end
end

%% Utility functions:

function s = appendStructs(varargin)
% s = appendStructs(s1, s2,...). Assign all fields in s1, s2,... into s.
if isempty(varargin)
    s = struct;
else
    s = varargin{1};
    for i = 2:numel(varargin)
        fromstr = varargin{i};
        fs = fieldnames(fromstr);
        for j = 1:numel(fs)
            s.(fs{j}) = fromstr.(fs{j});
        end
    end
end
end

function checkInputSize(inputShape, expectedShape, inputName)

if numel(expectedShape)==0
    % The input is a scalar
    if ~isequal(inputShape, [1 1])
        inputSizeStr = makeSizeString(inputShape);
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, "[1,1]", inputSizeStr));
    end
elseif numel(expectedShape)==1
    % The input is a vector
    if ~shapeIsColumnVector(inputShape) || ~iSizesMatch({inputShape(1)}, expectedShape)
        expectedShape{2} = 1;
        expectedSizeStr = makeSizeString(expectedShape);
        inputSizeStr = makeSizeString(inputShape);
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, expectedSizeStr, inputSizeStr));
    end
else
    % The input has 2 dimensions or more
    
    % The input dimensions have been reversed; flip them back to compare to the
    % expected ONNX shape.
    inputShape = fliplr(inputShape);
    
    % If the expected shape has fewer dims than the input shape, error.
    if numel(expectedShape) < numel(inputShape)
        expectedSizeStr = strjoin(["[", strjoin(string(expectedShape), ","), "]"], "");
        error(message('nnet_cnn_onnx:onnx:InputHasGreaterNDims', inputName, expectedSizeStr));
    end
    
    % Prepad the input shape with trailing ones up to the number of elements in
    % expectedShape
    inputShape = num2cell([ones(1, numel(expectedShape) - length(inputShape)) inputShape]);
    
    % Find the number of variable size dimensions in the expected shape
    numVariableInputs = sum(cellfun(@(x) isa(x, 'char') || isa(x, 'string'), expectedShape));
    
    % Find the number of input dimensions that are not in the expected shape
    % and cannot be represented by a variable dimension
    nonMatchingInputDims = setdiff(string(inputShape), string(expectedShape));
    numNonMatchingInputDims  = numel(nonMatchingInputDims) - numVariableInputs;
    
    expectedSizeStr = makeSizeString(expectedShape);
    inputSizeStr = makeSizeString(inputShape);
    if numNonMatchingInputDims == 0 && ~iSizesMatch(inputShape, expectedShape)
        % The actual and expected input dimensions match, but in
        % a different order. The input needs to be permuted.
        error(message('nnet_cnn_onnx:onnx:InputNeedsPermute',inputName, expectedSizeStr, inputSizeStr));
    elseif numNonMatchingInputDims > 0
        % The actual and expected input sizes do not match.
        error(message('nnet_cnn_onnx:onnx:InputNeedsResize',inputName, expectedSizeStr, inputSizeStr));
    end
end
end

function doesMatch = iSizesMatch(inputShape, expectedShape)
% Check whether the input and expected shapes match, in order.
% Size elements match if (1) the elements are equal, or (2) the expected
% size element is a variable (represented by a character vector or string)
doesMatch = true;
for i=1:numel(inputShape)
    if ~(isequal(inputShape{i},expectedShape{i}) || ischar(expectedShape{i}) || isstring(expectedShape{i}))
        doesMatch = false;
        return
    end
end
end

function sizeStr = makeSizeString(shape)
sizeStr = strjoin(["[", strjoin(string(shape), ","), "]"], "");
end

function isVec = shapeIsColumnVector(shape)
if numel(shape) == 2 && shape(2) == 1
    isVec = true;
else
    isVec = false;
end
end
function X = makeUnlabeledDlarray(X)
% Make numeric X into an unlabelled dlarray
if isa(X, 'dlarray')
    X = stripdims(X);
elseif isnumeric(X)
    if ~(isa(X,'single') || isa(X,'double'))
        % Make ints double so they can combine with anything without
        % reducting precision
        X = double(X);
    end
    X = dlarray(X);
end
end

function [Vars, NumDims] = packageVariables(params, inputNames, inputValues, inputNumDims)
% inputNames, inputValues are cell arrays. inputRanks is a numeric vector.
Vars = appendStructs(params.Learnables, params.Nonlearnables, params.State);
NumDims = params.NumDimensions;
% Add graph inputs
for i = 1:numel(inputNames)
    Vars.(inputNames{i}) = inputValues{i};
    NumDims.(inputNames{i}) = inputNumDims(i);
end
end

function X = permuteInputVar(X, userDataPerm, onnxNDims)
% Returns reverse-ONNX ordering
if onnxNDims == 0
    return;
elseif onnxNDims == 1 && isvector(X)
    X = X(:);
    return;
elseif isnumeric(userDataPerm)
    % Permute into reverse ONNX ordering
    perm = fliplr(userDataPerm);
elseif isequal(userDataPerm, 'auto') && onnxNDims == 4
    % Permute MATLAB HWCN to reverse onnx (WHCN)
    perm = [2 1 3 4];
else
    % userDataPerm is either 'none' or 'auto' with no default, which means
    % it's already in onnx ordering, so just make it reverse onnx
    perm = max(2,onnxNDims):-1:1;
end
X = permute(X, perm);
end
function Y = permuteOutputVar(Y, userDataPerm, onnxNDims)
switch onnxNDims
    case 0
        perm = [];
    case 1
        if isnumeric(userDataPerm)
            % Use the user's permutation because Y is a column vector which
            % already matches ONNX.
            perm = userDataPerm;
        elseif isequal(userDataPerm, 'auto')
            % Treat the 1D onnx vector as a 2D column and transpose it
            perm = [2 1];
        else
            % userDataPerm is 'none'. Leave Y alone because it already
            % matches onnx.
            perm = [];
        end
    otherwise
        % ndims >= 2
        if isnumeric(userDataPerm)
            % Use the inverse of the user's permutation. This is not just the
            % flip of the permutation vector.
            perm = onnxNDims + 1 - userDataPerm;
        elseif isequal(userDataPerm, 'auto')
            if onnxNDims == 2
                % Permute reverse ONNX CN to DLT CN (do nothing)
                perm = [];
            elseif onnxNDims == 4
                % Permute reverse onnx (WHCN) to MATLAB HWCN
                perm = [2 1 3 4];
            else
                % User wants the output in ONNX ordering, so just reverse it from
                % reverse onnx
                perm = onnxNDims:-1:1;
            end
        else
            % userDataPerm is 'none', so just make it reverse onnx
            perm = onnxNDims:-1:1;
        end
end
if ~isempty(perm)
    Y = permute(Y, perm);
end
end

function s = updateStruct(s, t)
% Set all existing fields in s from fields in t, ignoring extra fields in t.
for name = transpose(fieldnames(s))
    s.(name{1}) = t.(name{1});
end
end
